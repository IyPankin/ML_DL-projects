{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "## HW 4: Policy gradient\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "XPKYrIlai2Sf",
        "outputId": "9f0005cf-f7cc-480e-90ea-0278fac45cae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e3d64cbf200>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8_pYr7PZi2Sn"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(state_dim[0], 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, n_actions)\n",
        ")\n",
        "assert model is not None, \"model is not defined\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oeaj8i_a68aY",
        "outputId": "e40aeece-cca3-44ec-8abc-fe7da2442df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example_states_batch.shape: (5, 4)\n",
            "example_logits.shape: torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "# do not change the code block below\n",
        "batch_size_for_test = 5\n",
        "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
        "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
        "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
        "\n",
        "example_logits = model(torch.from_numpy(example_states_batch))\n",
        "print(f\"example_logits.shape: {example_logits.shape}\")\n",
        "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
        "# do not change the code block above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predicting the action probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
        "\n",
        "So, here gradient calculation is not needed.\n",
        "\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "\n",
        "Also, `.detach()` can be used instead, but there is a difference:\n",
        "\n",
        "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    with torch.no_grad():\n",
        "      states_tensor = torch.tensor(states, dtype=torch.float32)\n",
        "      logits = model(states_tensor)\n",
        "      probs = torch.softmax(logits, dim=-1).numpy()\n",
        "\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Obkl_jCii2Sv"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
        "test_probas = predict_probs(test_states, model)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]), model)[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5sdENWJAi2Sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a7f2e7-3084-4ee4-bf65-7c229633c3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([-0.01510479, -0.02946524,  0.03711469,  0.00605706], dtype=float32), array([-0.0156941 , -0.22509928,  0.03723583,  0.31021523], dtype=float32), array([-0.02019608, -0.03052708,  0.04344013,  0.02950405], dtype=float32), array([-0.02080663,  0.16394585,  0.04403022, -0.24916278], dtype=float32), array([-0.01752771,  0.35841227,  0.03904696, -0.527639  ], dtype=float32), array([-0.01035946,  0.16276331,  0.02849418, -0.22291236], dtype=float32), array([-0.0071042 , -0.03275407,  0.02403593,  0.07862082], dtype=float32), array([-0.00775928, -0.2282122 ,  0.02560835,  0.37878922], dtype=float32), array([-0.01232352, -0.0334631 ,  0.03318413,  0.09428938], dtype=float32), array([-0.01299278,  0.1611679 ,  0.03506992, -0.18774205], dtype=float32), array([-0.00976943, -0.03443779,  0.03131508,  0.11579451], dtype=float32), array([-0.01045818,  0.1602218 ,  0.03363097, -0.16684659], dtype=float32), array([-0.00725375,  0.3548466 ,  0.03029404, -0.4487331 ], dtype=float32), array([-1.5681419e-04,  5.4952723e-01,  2.1319376e-02, -7.3171496e-01],\n",
            "      dtype=float32), array([ 0.01083373,  0.74434817,  0.00668508, -1.0176126 ], dtype=float32), array([ 0.0257207 ,  0.5491378 , -0.01366717, -0.72283804], dtype=float32), array([ 0.03670345,  0.74444604, -0.02812394, -1.0197911 ], dtype=float32), array([ 0.05159237,  0.54971   , -0.04851976, -0.7360699 ], dtype=float32), array([ 0.06258657,  0.3552906 , -0.06324115, -0.45904326], dtype=float32), array([ 0.06969238,  0.5512468 , -0.07242202, -0.7709705 ], dtype=float32), array([ 0.08071732,  0.74728674, -0.08784143, -1.085533  ], dtype=float32), array([ 0.09566306,  0.5534264 , -0.10955209, -0.82165587], dtype=float32), array([ 0.10673158,  0.74986315, -0.1259852 , -1.1466885 ], dtype=float32), array([ 0.12172884,  0.55659103, -0.14891897, -0.89601976], dtype=float32), array([ 0.13286066,  0.75338405, -0.16683938, -1.2315627 ], dtype=float32), array([ 0.14792834,  0.95021164, -0.19147062, -1.5715281 ], dtype=float32)] [0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0] [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)\n",
        "print(states, actions, rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session\n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "    cumulative_rewards = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        cumulative_rewards.append(G)\n",
        "    cumulative_rewards.reverse() # reverse to get the original order\n",
        "\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "\n",
        "    return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DX39wcUi2S3",
        "outputId": "8766c96e-b775-4745-e02c-c0ec5ef96a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
        "\n",
        "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
        "\n",
        "$$\n",
        "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
        "$$\n",
        "where $\\lambda$ is the `entropy_coef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISNs5yly68aa"
      },
      "source": [
        "This function might be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kRrgdRy468aa"
      },
      "outputs": [],
      "source": [
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the loss for the REINFORCE algorithm.\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(log_probs * to_one_hot(actions, n_actions), dim=1) # [batch,]\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns)  # a number\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = -torch.sum(probs * log_probs, dim=-1).mean()\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    loss = -J_hat - entropy_coef * entropy\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckHj5sXBi2TE",
        "outputId": "f5a095ce-79a4-4212-83ee-b309feae8c2e",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2359954175.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:24.500\n",
            "mean reward:30.110\n",
            "mean reward:32.860\n",
            "mean reward:36.560\n",
            "mean reward:32.630\n",
            "mean reward:37.550\n",
            "mean reward:58.890\n",
            "mean reward:110.490\n",
            "mean reward:156.380\n",
            "mean reward:165.010\n",
            "mean reward:196.260\n",
            "mean reward:245.540\n",
            "mean reward:301.060\n",
            "mean reward:233.780\n",
            "mean reward:308.780\n",
            "mean reward:208.670\n",
            "mean reward:275.660\n",
            "mean reward:341.610\n",
            "mean reward:310.010\n",
            "mean reward:135.910\n",
            "mean reward:122.290\n",
            "mean reward:112.500\n",
            "mean reward:125.950\n",
            "mean reward:100.120\n",
            "mean reward:102.800\n",
            "mean reward:155.970\n",
            "mean reward:263.700\n",
            "mean reward:240.410\n",
            "mean reward:148.670\n",
            "mean reward:156.880\n",
            "mean reward:245.080\n",
            "mean reward:318.630\n",
            "mean reward:183.150\n",
            "mean reward:111.700\n",
            "mean reward:156.020\n",
            "mean reward:155.240\n",
            "mean reward:146.570\n",
            "mean reward:97.260\n",
            "mean reward:108.540\n",
            "mean reward:96.540\n",
            "mean reward:126.400\n",
            "mean reward:299.370\n",
            "mean reward:117.670\n",
            "mean reward:105.680\n",
            "mean reward:101.200\n",
            "mean reward:98.290\n",
            "mean reward:131.560\n",
            "mean reward:152.170\n",
            "mean reward:620.840\n",
            "mean reward:341.420\n",
            "mean reward:105.770\n",
            "mean reward:110.340\n",
            "mean reward:124.280\n",
            "mean reward:122.060\n",
            "mean reward:109.990\n",
            "mean reward:114.820\n",
            "mean reward:110.830\n",
            "mean reward:86.210\n",
            "mean reward:90.770\n",
            "mean reward:97.860\n",
            "mean reward:88.410\n",
            "mean reward:79.550\n",
            "mean reward:91.880\n",
            "mean reward:86.360\n",
            "mean reward:105.230\n",
            "mean reward:115.170\n",
            "mean reward:409.820\n",
            "mean reward:344.200\n",
            "mean reward:106.660\n",
            "mean reward:141.180\n",
            "mean reward:123.050\n",
            "mean reward:125.680\n",
            "mean reward:228.870\n",
            "mean reward:221.230\n",
            "mean reward:218.210\n",
            "mean reward:226.800\n",
            "mean reward:420.300\n",
            "mean reward:116.980\n",
            "mean reward:107.730\n",
            "mean reward:103.430\n",
            "mean reward:92.940\n",
            "mean reward:130.960\n",
            "mean reward:135.700\n",
            "mean reward:372.590\n",
            "mean reward:356.150\n",
            "mean reward:191.030\n",
            "mean reward:204.160\n",
            "mean reward:212.670\n",
            "mean reward:110.860\n",
            "mean reward:111.480\n",
            "mean reward:105.060\n",
            "mean reward:103.810\n",
            "mean reward:111.730\n",
            "mean reward:108.710\n",
            "mean reward:126.790\n",
            "mean reward:151.130\n",
            "mean reward:167.370\n",
            "mean reward:267.300\n",
            "mean reward:298.860\n",
            "mean reward:308.390\n",
            "mean reward:205.820\n",
            "mean reward:151.520\n",
            "mean reward:167.970\n",
            "mean reward:169.290\n",
            "mean reward:124.680\n",
            "mean reward:120.320\n",
            "mean reward:126.410\n",
            "mean reward:123.190\n",
            "mean reward:116.500\n",
            "mean reward:117.860\n",
            "mean reward:109.260\n",
            "mean reward:108.680\n",
            "mean reward:96.210\n",
            "mean reward:106.130\n",
            "mean reward:107.740\n",
            "mean reward:114.740\n",
            "mean reward:111.080\n",
            "mean reward:129.440\n",
            "mean reward:193.460\n",
            "mean reward:133.580\n",
            "mean reward:109.880\n",
            "mean reward:118.890\n",
            "mean reward:148.210\n",
            "mean reward:156.990\n",
            "mean reward:159.040\n",
            "mean reward:259.910\n",
            "mean reward:136.750\n",
            "mean reward:125.720\n",
            "mean reward:229.800\n",
            "mean reward:275.400\n",
            "mean reward:138.540\n",
            "mean reward:259.140\n",
            "mean reward:134.190\n",
            "mean reward:274.060\n",
            "mean reward:821.430\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(500):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 800:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "### Watch the video of your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-9z_FBf968ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.save_video import save_video\n",
        "\n",
        "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "n_actions = env_for_video.action_space.n\n",
        "\n",
        "episode_index = 0\n",
        "step_starting_index = 0\n",
        "\n",
        "obs, info = env_for_video.reset()\n",
        "\n",
        "for step_index in range(800):\n",
        "    probs = predict_probs(np.array([obs]), model)[0]\n",
        "    action = np.random.choice(n_actions, p=probs)\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done or step_index == 799:\n",
        "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
        "        frames = env_for_video.render()\n",
        "        os.makedirs(\"videos\", exist_ok=True)\n",
        "        save_video(\n",
        "            frames, \"videos\",\n",
        "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
        "            step_starting_index=step_starting_index,\n",
        "            episode_index=episode_index,\n",
        "        )\n",
        "        episode_index += 1\n",
        "        step_starting_index = step_index + 1\n",
        "        obs, info = env_for_video.reset()\n",
        "\n",
        "env_for_video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaVRkTM068ab"
      },
      "source": [
        "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E38KIiDS68ab"
      },
      "source": [
        "## Bonus part (no points, just for the interested ones)\n",
        "\n",
        "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
        "\n",
        "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "aFq4e1S668ab",
        "outputId": "dc43197a-3a21-4237-a789-8489764c98a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e3c620f2960>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGiCAYAAABd6zmYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJVdJREFUeJzt3X94VPWB7/HPmZnM5BczIWASEVLZq1dN+WEFham722clS7Rpqyvuui6PRZe1Vxp8RFzvyq7ird3nhgf32mrXH3vbrnjbKl16F61UtDxBQ13CDwPUAJrVW9pkhUkQzEwSkkky871/sEwdRZqQSc53hvfreeZ5yDlnTr5zYObNOXPmjGOMMQIAwEIetwcAAMCnIVIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGu5FqknnnhCF154ofLz8zVv3jzt2rXLraEAACzlSqR+/OMfa+XKlXrooYe0Z88ezZ49WzU1Ners7HRjOAAASzluXGB23rx5uvLKK/WP//iPkqRkMqlp06bprrvu0v333z/ewwEAWMo33r9wYGBAzc3NWrVqVWqax+NRdXW1mpqaTnufeDyueDye+jmZTOr48eOaNGmSHMcZ8zEDADLLGKPu7m5NmTJFHs+nH9Qb90h98MEHSiQSKi8vT5teXl6ud95557T3qa+v1ze+8Y3xGB4AYBy1t7dr6tSpnzp/3CN1NlatWqWVK1emfo5Go6qsrFR7e7uCwaCLIwMAnI1YLKZp06ZpwoQJZ1xu3CM1efJkeb1edXR0pE3v6OhQRUXFae8TCAQUCAQ+MT0YDBIpAMhiv+stm3E/u8/v92vOnDlqaGhITUsmk2poaFA4HB7v4QAALObK4b6VK1dqyZIlmjt3rq666ip9+9vfVm9vr26//XY3hgMAsJQrkbr55pt19OhRrV69WpFIRJdffrleeeWVT5xMAQA4t7nyOanRisViCoVCikajvCcFAFlouK/jXLsPAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLVGHKlt27bpy1/+sqZMmSLHcfTCCy+kzTfGaPXq1Tr//PNVUFCg6upqvfvuu2nLHD9+XIsXL1YwGFRJSYmWLl2qnp6eUT0QAEDuGXGkent7NXv2bD3xxBOnnb927Vo9/vjjevrpp7Vz504VFRWppqZG/f39qWUWL16sAwcOaMuWLdq0aZO2bdumr33ta2f/KAAAucmMgiSzcePG1M/JZNJUVFSYRx55JDWtq6vLBAIB8/zzzxtjjDl48KCRZHbv3p1aZvPmzcZxHPP+++8P6/dGo1EjyUSj0dEMHwDgkuG+jmf0PalDhw4pEomouro6NS0UCmnevHlqamqSJDU1NamkpERz585NLVNdXS2Px6OdO3eedr3xeFyxWCztBgDIfRmNVCQSkSSVl5enTS8vL0/Ni0QiKisrS5vv8/lUWlqaWubj6uvrFQqFUrdp06ZlctgAAEtlxdl9q1atUjQaTd3a29vdHhIAYBxkNFIVFRWSpI6OjrTpHR0dqXkVFRXq7OxMmz80NKTjx4+nlvm4QCCgYDCYdgMA5L6MRmr69OmqqKhQQ0NDalosFtPOnTsVDoclSeFwWF1dXWpubk4ts3XrViWTSc2bNy+TwwEAZDnfSO/Q09Oj9957L/XzoUOHtG/fPpWWlqqyslIrVqzQ3//93+viiy/W9OnT9eCDD2rKlCm64YYbJEmXXXaZrr32Wt1xxx16+umnNTg4qOXLl+vP//zPNWXKlIw9MABADhjpaYOvvfaakfSJ25IlS4wxJ09Df/DBB015ebkJBAJmwYIFprW1NW0dx44dM7fccospLi42wWDQ3H777aa7uzvjpy4CAOw03NdxxxhjXGzkWYnFYgqFQopGo7w/BQBZaLiv41lxdh8A4NxEpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1vK5PYDReP7551VQUOD2MAAAI9TX1zes5bI6UsYYGWPcHgYAYISG+9rtmCx8lY/FYgqFQopGowoGg24PBwAwQsN9Hec9KQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaRAoAYC0iBQCwFpECAFiLSAEArEWkAADWIlIAAGsRKQCAtYgUAMBaI4pUfX29rrzySk2YMEFlZWW64YYb1NramrZMf3+/6urqNGnSJBUXF2vRokXq6OhIW6atrU21tbUqLCxUWVmZ7rvvPg0NDY3+0QAAcsqIItXY2Ki6ujrt2LFDW7Zs0eDgoBYuXKje3t7UMvfcc49eeuklbdiwQY2NjTp8+LBuvPHG1PxEIqHa2loNDAxo+/btevbZZ7Vu3TqtXr06c48KAJAbzCh0dnYaSaaxsdEYY0xXV5fJy8szGzZsSC3z9ttvG0mmqanJGGPMyy+/bDwej4lEIqllnnrqKRMMBk08Hh/W741Go0aSiUajoxk+AMAlw30dH9V7UtFoVJJUWloqSWpubtbg4KCqq6tTy1x66aWqrKxUU1OTJKmpqUkzZ85UeXl5apmamhrFYjEdOHDgtL8nHo8rFoul3QAAue+sI5VMJrVixQpdffXVmjFjhiQpEonI7/erpKQkbdny8nJFIpHUMh8N1Kn5p+adTn19vUKhUOo2bdq0sx02ACCLnHWk6urqtH//fq1fvz6T4zmtVatWKRqNpm7t7e1j/jsBAO7znc2dli9frk2bNmnbtm2aOnVqanpFRYUGBgbU1dWVtjfV0dGhioqK1DK7du1KW9+ps/9OLfNxgUBAgUDgbIYKAMhiI9qTMsZo+fLl2rhxo7Zu3arp06enzZ8zZ47y8vLU0NCQmtba2qq2tjaFw2FJUjgcVktLizo7O1PLbNmyRcFgUFVVVaN5LACAHDOiPam6ujo999xzevHFFzVhwoTUe0ihUEgFBQUKhUJaunSpVq5cqdLSUgWDQd11110Kh8OaP3++JGnhwoWqqqrSrbfeqrVr1yoSieiBBx5QXV0de0sAgDSOMcYMe2HHOe30Z555Rrfddpukkx/mvffee/X8888rHo+rpqZGTz75ZNqhvN/85jdatmyZXn/9dRUVFWnJkiVas2aNfL7hNTMWiykUCikajSoYDA53+AAASwz3dXxEkbIFkQKA7Dbc13Gu3QcAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKzlc3sAAH7LGPOp8xzHGceRAHYgUoAFjBnS0NAxxWKvqqtrk/r7DyiR6JHPN1lFRXM1ceKfqbDwCnm9ITkOB0Bw7iBSgMuSyT51db2gjo7HdOLELkm/3ZsaHGxTX98eHTv2A4VC16qsbKWKi69mrwrnDP5LBrjImKSOHv2u2tvv0YkTO/XRQKUv16euro1qa/u6enpeP+NhQSCXECnAJcYM6dixdTp8eLWGhjqGdZ/+/ha1td2tnp5/kzHJMR4h4D4iBbikt3enIpH/qWQyOqL79fe36MiR/6FEomtsBgZYhEgBLkgm44pGNyse/39ndf/u7gadOLGXw37IeUQKcMHg4H+oo2PtqNbR1vb1DI0GsBeRAlxgjJExg6NcR3+GRgPYi0gB48wYoziH6YBhIVKAC7703nujXkfCSL1JzvBDbiNSgAsGMrAnlZRRTyKRgdEA9iJSgAs+1ERtUu2o1vF9c5u62ZNCjiNSgAt6VaTX9EfqUuis7t+madqqLyjGnhRyHJECXLJD8/UT3aTBEV5C86gm6wl9XR+YiexJIecRKcAFhR6P4srXD3SrNql22KGKaoK+p7/SL/SHGjCOjg0NjfFIAXcRKcAF/23yZEknD/t9W/donW7T+zr/Uy4vKw3Kp1ZdrH/QX+v/apEGFFAsmdTL0ZFdUgnINnxVB+CCUt+pp56jXhVrnW7Tbs3VNXpNn9MeTdV/qED9iimoQ5quf9PVekO/r1/p9yTxNR04dxApwAWTvN60n+PK1x7N0QHNUKFOKE+D8iiphLwakF+9KtKQ8lwaLeAeIgW4YJLvdE89R3HlK678cR8PYCvekwJcMPG0kQLwcUQKcEEgQ1//biS+rgM5jUgB48zJUKCkk5dXysQllgBbESkgi/Unk1xRHTmNSAFZrC+ZVD9XnUAOI1JAFusnUshxRApwQdDr1R8UF496Pf18gSJyHJECXFDg8ei/+P2jXs+xoSF9yJXQkcOIFOACr6QJH7vqxNlojcf1Xjw++gEBliJSgAu8jqNQBiIF5DoiBbjAR6SAYSFSgAs8OvmdUgDOjGcJ4ALHceTJ4JUngFxFpIAslzSG6/chZxEpIMudSCY/9Rt9gWxHpIAsF0skxDUnkKuIFOCSyT5fRs7wiyYSSnK4DzmKSAEuuSQ/Xxdm4KoTPckke1LIWUQKcEmRx6P8DJyGHuVwH3IYkQJcUug4ys/AaegtfX0a4EroyFFECnBJgdebkT2pN0+c4EroyFlECnBJoeMowAd6gTMiUoBL8jwe+YgUcEZECgBgrRFF6qmnntKsWbMUDAYVDAYVDoe1efPm1Pz+/n7V1dVp0qRJKi4u1qJFi9TR0ZG2jra2NtXW1qqwsFBlZWW67777NDQ0lJlHAwDIKSOK1NSpU7VmzRo1NzfrzTff1DXXXKPrr79eBw4ckCTdc889eumll7RhwwY1Njbq8OHDuvHGG1P3TyQSqq2t1cDAgLZv365nn31W69at0+rVqzP7qIBzTIITJ5CjHDPKK1OWlpbqkUce0U033aTzzjtPzz33nG666SZJ0jvvvKPLLrtMTU1Nmj9/vjZv3qwvfelLOnz4sMrLyyVJTz/9tP7mb/5GR48elX+YH2yMxWIKhUKKRqMKBoOjGT7gqj/71a+04cMPR72edz77WV2Sn5+BEQHjY7iv42f9nlQikdD69evV29urcDis5uZmDQ4Oqrq6OrXMpZdeqsrKSjU1NUmSmpqaNHPmzFSgJKmmpkaxWCy1N3Y68XhcsVgs7Qbkgj8tKVFBBk6e6OKQOXLUiCPV0tKi4uJiBQIB3Xnnndq4caOqqqoUiUTk9/tVUlKStnx5ebkikYgkKRKJpAXq1PxT8z5NfX29QqFQ6jZt2rSRDhuwUqnPl5HvlepKJDIwGsA+I47UJZdcon379mnnzp1atmyZlixZooMHD47F2FJWrVqlaDSaurW3t4/p7wPGS4nXm5FTbIkUcpVvpHfw+/266KKLJElz5szR7t279dhjj+nmm2/WwMCAurq60vamOjo6VFFRIUmqqKjQrl270tZ36uy/U8ucTiAQUCAQGOlQAetNzNCe1Icc7kOOGvV/4pLJpOLxuObMmaO8vDw1NDSk5rW2tqqtrU3hcFiSFA6H1dLSos7OztQyW7ZsUTAYVFVV1WiHAmSdTO1JNXR3Z2AtgH1GtCe1atUqXXfddaqsrFR3d7eee+45vf7663r11VcVCoW0dOlSrVy5UqWlpQoGg7rrrrsUDoc1f/58SdLChQtVVVWlW2+9VWvXrlUkEtEDDzyguro69pRwTgp6vcrENSda+voysBbAPiOKVGdnp7761a/qyJEjCoVCmjVrll599VX98R//sSTpW9/6ljwejxYtWqR4PK6amho9+eSTqft7vV5t2rRJy5YtUzgcVlFRkZYsWaKHH344s48KyBJc8gU4s1F/TsoNfE4KuSJpjM775S91fJQnPlwSCOidGTMyNCpg7I3556QAABhrRAoAYC0iBbgsU1/Wkcy+I/fA70SkABc5kv5q8uRRrychqZevkEcOIlKAyy4Y5oWVzyRhjLq56gRyEJECXDbR6x31OoaMYU8KOYlIAS6b5Bvx1ck+ISEpxp4UchCRAlxWmoE9qYQx6mFPCjmISAEuK/CM/ml4PJFQI9fvQw4iUoCLHMeRk4GroA8ao6NcCR05iEgBAKxFpAAA1iJSAABrESnAZRM8HlXl57s9DMBKRApwWdDr1WczECkjrt+H3EOkAJflOY6CGfisVF8yqQEihRxDpACX+RxHE4gUcFpECnCZL4N7UnGuOoEcQ6QAl/kkFWXgqhN9xrAnhZxDpACXOY4jbwauOsHhPuQiIgXkiB29vXqnv9/tYQAZRaSAHDFgjAbZk0KOIVIAAGsRKcAChR6P/Bl4XwrINUQKsMBVhYX6Pb/f7WEA1iFSgAWKPB75M3AauiQZ3pdCDiFSgAWKvF7lZ+BwX4JAIccQKcACmdqTiiUSIlPIJUQKsEChx6NABvakYskkkUJOIVKABQKOI18GIhVNJMTV+5BLfG4PAMDJSyN9mjJFNFtvaYoOq1C96lOBIjpfLZqp93WBpN/eN5ZInDxxgtPZkSOIFGAlo5Ci+qJ+pi/pZ6pQRIU6IZ+GNCSv+lSoozpPr6hGP9X1OqZJkhx9MDTEnhRyCpECLDRN7fprPaKwdsiR0Uf3i/KUUJ66NUHdWqanFVaTHtF/17v6r/rBsWN6dOpU5WfodHbAbfxLBixzgdp1tx5TWDvk+VigPsqR5JHR57RPK/WoLtSvOGkCOYdIAZb4SiikEueE/kLP62q9Ic8wk+NIukJ7dJvWKaTo2A4SGGdECrDEZ/x+zdZb+jP9i/KUGNF9vUqqVi/r89ouh/0p5BAiBVhiks/REv3gUw/v/S6OpJv1Y+UrnslhAa4iUoAlSr0ezdT+Ua3jMr0tr4YyNCLAfUQKsESp1ydPBj7fNMT1+5BDiBRgiZDPe9aH+j6qKzGy97MAmxEpwBKZuCySkdGxIQ73IXcQKcASSTnaq8tHtY4WzVQnO1LIIUQKsERSHj2nxWd9ArmRtEF/qiNDXEgGuYNIAdZw1KKZ+pH+QoMjvGLZkLx6Udfr3/R5vRcfGKPxAeOPSAEWOaFC/Ytu1jb9oZLDPI3CSHpTc/V/9FXFFNQLXV1jOkZgPBEpwCJex9FhXaDHdLd+od9X4gwXRzKSEvJot+bqf+letekz4zlUYFxw8BqwhEfSHZMm6R86O3VYU/SQHta1ekW12qQL9L6K1as8DWpIPvWqSBFVaLOu1c/0JXWpxO3hA2OCSAGWcCRN8vlSP/Vogn6iP1WjvqAZ2p/6Tqk+5atD5Tqoz+qIzpcy8ukqwE5ECrBIyPfJp+RRlek1XTOi9Rhjzvhtv0C24D0pwBKOpIle76jXk5DUn+T7eZEbiBRgkVAGIjVkjLqJFHIEkQIskolIDRqjHq7fhxxBpABLOI4jbwbeRxo0RjH2pJAjiBSQY9iTQi4hUkCOiQwOamt3t9vDADKCSAE5JiHpBIf7kCOIFADAWkQKsIhPUgEfwgVSiBRgkQv8fl0zYYLbwwCsQaQAi+Q5jgo9o39aJiUlzdl+fSJgDyIFWCTPcVScgQ/0nkgmNUikkAOIFGARX4b2pHoSCcWJFHIAkQIs4pNUlIFI9SaTGiBSyAGjejasWbNGjuNoxYoVqWn9/f2qq6vTpEmTVFxcrEWLFqmjoyPtfm1tbaqtrVVhYaHKysp03333aWhoaDRDAXJCpt6T6kkmFeezUsgBZ/1s2L17t/7pn/5Js2bNSpt+zz336KWXXtKGDRvU2Niow4cP68Ybb0zNTyQSqq2t1cDAgLZv365nn31W69at0+rVq8/+UQA5wnEceTJwCnprf7+O8h8/5ICzilRPT48WL16s7373u5o4cWJqejQa1fe//309+uijuuaaazRnzhw988wz2r59u3bs2CFJ+vnPf66DBw/qhz/8oS6//HJdd911+uY3v6knnnhCAwMDmXlUwDnu0MCAjhEp5ICzilRdXZ1qa2tVXV2dNr25uVmDg4Np0y+99FJVVlaqqalJktTU1KSZM2eqvLw8tUxNTY1isZgOHDhw2t8Xj8cVi8XSbgCA3Dfir49fv3699uzZo927d39iXiQSkd/vV0lJSdr08vJyRSKR1DIfDdSp+afmnU59fb2+8Y1vjHSoAIAsN6I9qfb2dt1999360Y9+pPz8/LEa0yesWrVK0Wg0dWtvbx+33w0AcM+IItXc3KzOzk5dccUV8vl88vl8amxs1OOPPy6fz6fy8nINDAyoq6sr7X4dHR2qqKiQJFVUVHzibL9TP59a5uMCgYCCwWDaDchVk73ejFy/z0gynIaOLDeiSC1YsEAtLS3at29f6jZ37lwtXrw49ee8vDw1NDSk7tPa2qq2tjaFw2FJUjgcVktLizo7O1PLbNmyRcFgUFVVVRl6WED2mltUpCl+/6jX08cp6MgBI3pPasKECZoxY0batKKiIk2aNCk1fenSpVq5cqVKS0sVDAZ11113KRwOa/78+ZKkhQsXqqqqSrfeeqvWrl2rSCSiBx54QHV1dQoEAhl6WED2KvJ4lJeBPakPEwkZSVxTHdlsxCdO/C7f+ta35PF4tGjRIsXjcdXU1OjJJ59Mzfd6vdq0aZOWLVumcDisoqIiLVmyRA8//HCmhwJkpcIMRer40JA42Ids55gsPGgdi8UUCoUUjUZ5fwo5p3NwUNe99572nDgxqvU8dP75euD88+Xj+6lgoeG+jnPtPsAyRR6P/BkIS/vAAHtSyHpECrBMgceTkb2fHx4/ztd1IOsRKcAyHsfhZAfgPxEpAIC1iBQAwFpECgBgLSIF5LAkJ04gyxEpwEKzCgoycvJENJHIwFoA9xApwEJ/UFws7yjXYSR9yBcfIssRKcBCE72jTdTJK6AfY08KWY5IARYKeb1yMnGRWfakkOWIFGChkNc76vekjKSjRApZjkgBFproG/0XFCQk/ej48dEPBnARkQIsVOTxZOTsPr72ENmOSAEW8vD1GoAkIgUAsBiRAgBYi0gBOSxhjAaSvDOF7EWkgBw2YIy6iRSyGJECLOSV9PvFxaNez6Ax6uWqE8hiRAqwkNdxNLugYNTriRujHvakkMWIFGAhR1JJBq7fN5hMEilkNSIFWCqUgUgdGRpS84kTGRgN4A4iBVgoU3tSJ5JJHR0cHP2AAJcQKcBCjjKzJwVkOyIFWKqISAFECrCR4zg8OQERKSDnJXXyW3qBbESkgBzXl0yKrz5EtiJSQI7rTiS4fh+yFpECclxPMqlBDvchSxEpwFIX+P26IgOXRiJSyGZECrBUscejsry8Ua/n1/G4YhzuQ5YiUoCl8hxHhZ7RP0X39vWpk6tOIEsRKcBSfsdRQQYiBWQzngGApfyOoyIihXMczwDAUpk63AdkM54BgKV8jqN8x8nY+rjqBLIRkQIs5TiOlKFIxTm7D1mKSAHngA8TCbeHAJwVIgWcA44PDYmDfchGRAo4BxxnTwpZikgBFsvUaRPsSSFbESnAYn80YYIqfL5Rr+d/f/CBOHUC2YhIARYLeb3yZ+CzUnFjOAUdWYlIARYr9Hg0+v0oIHsRKcBiEzwe5WXwA71AtiFSgMWKPB75iBTOYRxJACxW5PWmIuXot2f7pf3ZcT4x/ePzij2ek1ewALIMkQIslu84+oPiYv2e369Cj0eFXq+KPB4Vejwq+s/bqT8Xer0qdBwVe70nl/3IfN7bQrbi3y1gMcdx9J3KSreHAbiG96QAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALBWVn5VhzFGkhSLxVweCQDgbJx6/T71ev5psjJSx44dkyRNmzbN5ZEAAEaju7tboVDoU+dnZaRKS0slSW1tbWd8cOe6WCymadOmqb29XcFg0O3hWIvtNDxsp+FhOw2PMUbd3d2aMmXKGZfLykh5PCffSguFQvwjGIZgMMh2Gga20/CwnYaH7fS7DWcngxMnAADWIlIAAGtlZaQCgYAeeughBQIBt4diNbbT8LCdhoftNDxsp8xyzO86/w8AAJdk5Z4UAODcQKQAANYiUgAAaxEpAIC1sjJSTzzxhC688ELl5+dr3rx52rVrl9tDGlfbtm3Tl7/8ZU2ZMkWO4+iFF15Im2+M0erVq3X++eeroKBA1dXVevfdd9OWOX78uBYvXqxgMKiSkhItXbpUPT094/goxlZ9fb2uvPJKTZgwQWVlZbrhhhvU2tqatkx/f7/q6uo0adIkFRcXa9GiRero6Ehbpq2tTbW1tSosLFRZWZnuu+8+DQ0NjedDGVNPPfWUZs2alfrgaTgc1ubNm1Pz2Uant2bNGjmOoxUrVqSmsa3GiMky69evN36/3/zzP/+zOXDggLnjjjtMSUmJ6ejocHto4+bll182f/d3f2f+9V//1UgyGzduTJu/Zs0aEwqFzAsvvGB++ctfmq985Stm+vTppq+vL7XMtddea2bPnm127NhhfvGLX5iLLrrI3HLLLeP8SMZOTU2NeeaZZ8z+/fvNvn37zBe/+EVTWVlpenp6UsvceeedZtq0aaahocG8+eabZv78+ebzn/98av7Q0JCZMWOGqa6uNnv37jUvv/yymTx5slm1apUbD2lM/PSnPzU/+9nPzL//+7+b1tZW87d/+7cmLy/P7N+/3xjDNjqdXbt2mQsvvNDMmjXL3H333anpbKuxkXWRuuqqq0xdXV3q50QiYaZMmWLq6+tdHJV7Ph6pZDJpKioqzCOPPJKa1tXVZQKBgHn++eeNMcYcPHjQSDK7d+9OLbN582bjOI55//33x23s46mzs9NIMo2NjcaYk9skLy/PbNiwIbXM22+/bSSZpqYmY8zJ/wx4PB4TiURSyzz11FMmGAyaeDw+vg9gHE2cONF873vfYxudRnd3t7n44ovNli1bzBe+8IVUpNhWYyerDvcNDAyoublZ1dXVqWkej0fV1dVqampycWT2OHTokCKRSNo2CoVCmjdvXmobNTU1qaSkRHPnzk0tU11dLY/Ho507d477mMdDNBqV9NuLEzc3N2twcDBtO1166aWqrKxM204zZ85UeXl5apmamhrFYjEdOHBgHEc/PhKJhNavX6/e3l6Fw2G20WnU1dWptrY2bZtI/HsaS1l1gdkPPvhAiUQi7S9ZksrLy/XOO++4NCq7RCIRSTrtNjo1LxKJqKysLG2+z+dTaWlpaplckkwmtWLFCl199dWaMWOGpJPbwO/3q6SkJG3Zj2+n023HU/NyRUtLi8LhsPr7+1VcXKyNGzeqqqpK+/btYxt9xPr167Vnzx7t3r37E/P49zR2sipSwNmoq6vT/v379cYbb7g9FCtdcskl2rdvn6LRqH7yk59oyZIlamxsdHtYVmlvb9fdd9+tLVu2KD8/3+3hnFOy6nDf5MmT5fV6P3HGTEdHhyoqKlwalV1ObYczbaOKigp1dnamzR8aGtLx48dzbjsuX75cmzZt0muvvaapU6empldUVGhgYEBdXV1py398O51uO56alyv8fr8uuugizZkzR/X19Zo9e7Yee+wxttFHNDc3q7OzU1dccYV8Pp98Pp8aGxv1+OOPy+fzqby8nG01RrIqUn6/X3PmzFFDQ0NqWjKZVENDg8LhsIsjs8f06dNVUVGRto1isZh27tyZ2kbhcFhdXV1qbm5OLbN161Ylk0nNmzdv3Mc8FowxWr58uTZu3KitW7dq+vTpafPnzJmjvLy8tO3U2tqqtra2tO3U0tKSFvQtW7YoGAyqqqpqfB6IC5LJpOLxONvoIxYsWKCWlhbt27cvdZs7d64WL16c+jPbaoy4febGSK1fv94EAgGzbt06c/DgQfO1r33NlJSUpJ0xk+u6u7vN3r17zd69e40k8+ijj5q9e/ea3/zmN8aYk6egl5SUmBdffNG89dZb5vrrrz/tKeif+9znzM6dO80bb7xhLr744pw6BX3ZsmUmFAqZ119/3Rw5ciR1O3HiRGqZO++801RWVpqtW7eaN99804TDYRMOh1PzT50yvHDhQrNv3z7zyiuvmPPOOy+nThm+//77TWNjozl06JB56623zP33328cxzE///nPjTFsozP56Nl9xrCtxkrWRcoYY77zne+YyspK4/f7zVVXXWV27Njh9pDG1WuvvWYkfeK2ZMkSY8zJ09AffPBBU15ebgKBgFmwYIFpbW1NW8exY8fMLbfcYoqLi00wGDS333676e7uduHRjI3TbR9J5plnnkkt09fXZ77+9a+biRMnmsLCQvMnf/In5siRI2nr+fWvf22uu+46U1BQYCZPnmzuvfdeMzg4OM6PZuz85V/+pfnMZz5j/H6/Oe+888yCBQtSgTKGbXQmH48U22ps8FUdAABrZdV7UgCAcwuRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1vr/3jxC0gMMDv0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your brave and victorious code here.\n",
        "\n",
        "# Build a simple neural network that predicts policy logits.\n",
        "# Acrobot has a state space of size 6 and action space of size 3.\n",
        "acrobot_model = nn.Sequential(\n",
        "    nn.Linear(state_dim[0], 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, n_actions)\n",
        ")\n",
        "\n",
        "acrobot_optimizer = torch.optim.Adam(acrobot_model.parameters(), 1e-3)\n",
        "\n",
        "def generate_acrobot_session(env, t_max=1000):\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        action_probs = predict_probs(np.array([s]), acrobot_model)[0]\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards\n",
        "\n",
        "def train_on_acrobot_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = acrobot_model(states)\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "\n",
        "    acrobot_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    acrobot_optimizer.step()\n",
        "\n",
        "    return np.sum(rewards)\n",
        "\n",
        "# Training loop\n",
        "for i in range(1000): # You might need more iterations\n",
        "    rewards = [train_on_acrobot_session(*generate_acrobot_session(env), entropy_coef=1e-3) for _ in range(10)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "\n",
        "    if np.mean(rewards) > -120:\n",
        "        print(\"You Win!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "uFBNQhPgCzOc",
        "outputId": "07cbbd0e-25b9-4bfc-e1db-fc02a97dcffa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:-722.100\n",
            "mean reward:-318.200\n",
            "mean reward:-262.900\n",
            "mean reward:-219.800\n",
            "mean reward:-392.000\n",
            "mean reward:-310.400\n",
            "mean reward:-311.700\n",
            "mean reward:-440.500\n",
            "mean reward:-779.800\n",
            "mean reward:-803.200\n",
            "mean reward:-929.200\n",
            "mean reward:-772.500\n",
            "mean reward:-985.400\n",
            "mean reward:-1000.000\n",
            "mean reward:-1000.000\n",
            "mean reward:-1000.000\n",
            "mean reward:-1000.000\n",
            "mean reward:-1000.000\n",
            "mean reward:-1000.000\n",
            "mean reward:-1000.000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-660269771.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# You might need more iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_on_acrobot_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerate_acrobot_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-660269771.py\u001b[0m in \u001b[0;36mgenerate_acrobot_session\u001b[0;34m(env, t_max)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macrobot_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/getlimits.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0m_finfo_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finfo_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# most common path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}